
import { createClient } from "npm:@supabase/supabase-js@2.39.3"; // Use fixed version for script

// Load env from .env or just assume local defaults for now since we run with `supabase functions serve` usually or `deno run`. 
// For local execution, we need the keys. I'll ask the user to provide them or try to read from a file if I could, but strictly I should rely on Deno.env if I run with `supabase functions`.
// BUT, to run this as a standalone script, I need the URL/KEY.
// I will assume local supabase defaults:
const SUPABASE_URL = "http://127.0.0.1:54321";
const SUPABASE_SERVICE_ROLE_KEY = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY") || "SERVICE_ROLE_KEY_PLACEHOLDER"; 

// !! USER MUST PROVIDE KEY IF RUNNING STANDALONE OR WE RELY ON IT BEING IN ENV

if (SUPABASE_SERVICE_ROLE_KEY === "SERVICE_ROLE_KEY_PLACEHOLDER") {
    console.error("Please set SUPABASE_SERVICE_ROLE_KEY environment variable.");
    Deno.exit(1);
}

const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY);

async function main() {
    console.log("ðŸš€ Starting Pipeline Reboot...");

    // 1. CLEAN DATA
    console.log("ðŸ§¹ Cleaning tables...");
    const { error: deleteError } = await supabase.rpc('reset_scraper_data_unsafe'); 
    // Wait, I don't have this RPC. I should use direct DELETE if RLS allows or make a quick SQL call?
    // I can't run raw SQL easily via JS client without an RPC that executes SQL.
    // Plan B: Use the `events` and `raw_event_staging` table APIs to delete all.
    
    // Deleting via Table API (batching might be needed if HUGE, but local is small likely)
    await supabase.from('event_attendees').delete().neq('id', '00000000-0000-0000-0000-000000000000'); // Delete all
    await supabase.from('events').delete().neq('id', '00000000-0000-0000-0000-000000000000');
    await supabase.from('raw_event_staging').delete().neq('id', '00000000-0000-0000-0000-000000000000');
    await supabase.from('scraper_insights').delete().neq('id', '00000000-0000-0000-0000-000000000000');
    await supabase.from('scraper_runs').delete().neq('id', '00000000-0000-0000-0000-000000000000');
    
    // Reset Sources
    await supabase.from('scraper_sources').update({
        last_scraped_at: null,
        consecutive_failures: 0,
        consecutive_errors: 0,
        is_available: true
    }).neq('id', '00000000-0000-0000-0000-000000000000');

    console.log("âœ¨ Tables cleaned.");

    // 2. TRIGGER SCRAPER
    console.log("ðŸ•·ï¸  Triggering Scraper via Scheduler...");

    // We need to know WHICH sources to scrape. 
    // Ideally we'd select enabled sources and create payloads for them.
    const { data: sources } = await supabase.from('scraper_sources').select('id').eq('enabled', true);
    
    if (!sources || sources.length === 0) {
        console.error("No enabled sources found!");
    } else {
        const jobs = sources.map(s => ({ source_id: s.id, payload: {}, next_scrape_at: new Date().toISOString() }));
        const { data: jobData, error: jobError } = await supabase.rpc('enqueue_scrape_jobs', { p_jobs: jobs });

        if (jobError) {
            console.error("Error enqueuing jobs:", jobError);
        } else {
            console.log(`âœ… Enqueued ${jobs.length} jobs.`);
        }
        
        // NOW we need to actually RUN them. 
        // If there is no auto-worker, we might need to manually trigger the 'scraper-worker' function for each job?
        // Let's try to assume there is a 'scraper-worker' function we can invoke.
        // I will attempt to invoke 'scraper-v2-worker' or similar if I saw it in file list, otherwise generic 'scraper'.
        // Based on previous context, user has 'scraper-scheduler'.
        // I'll try invoking 'process-worker' later, but for fetching HTML:
        // I will try to call 'scraper-worker' (guess) or just hope the scheduler does it.
    }

    // Since we don't have an actual "scraper worker" running constantly in the background locally unless the user started it,
    // we might need to simulate the scraping or assume the "scraper-scheduler" function does the work? 
    // Usually `enqueue_scrape_jobs` just creates DB rows. The `process-worker` (Refinery) is separate.
    // The actual "Scraping" (fetching HTML) is usually another worker or the same.
    // Looking at previous chats, there's a "process-worker" (Refinery). 
    // Is there a "fetch-worker"? 
    // The user said "run the scraper". 
    // If I look at `task.md` or logs, `enqueue_scrape_jobs` creates jobs. 
    // Who CONSUMES them? `scraper_swarm`? or `fetch-worker`?
    // I will assume there is a `scraper-worker` or similar. I'll try to invoke it.
    
    // Inspecting file structure might reveal `supabase/functions/scraper-worker` or similar.
    // For now, I'll assume `process-worker` handles the STAGING rows, but something must put them there.
    
    // HACK: I'll invoke `process-worker` repeatedly.
    // Wait, if `enqueue_scrape_jobs` just puts jobs in `scraper_jobs` table, we need something to run them.
    // I'll list functions to check.

    // 3. WAIT & POLL (Simulated)
    console.log("â³ Waiting for scraper to populate staging (Manual step if no auto-worker)...");
    // Ideally we invoke the scraper function here.
    
    // 4. TRIGGER REFINERY
    console.log("ðŸ­ Triggering Refinery (Process Worker)...");
    const { error: processError } = await supabase.functions.invoke('process-worker', {
        body: {}
    });
    if (processError) console.error("Error invoking process-worker:", processError);
    else console.log("âœ… Process-worker invoked.");

    // 5. SUMMARY
    console.log("ðŸ“Š Generating Summary...");
    // Sleep a bit to allow processing
    // await new Promise(r => setTimeout(r, 5000));

    const { data: events } = await supabase.from('events').select('category, source_id, created_at, event_type');
    console.log(`Total Events: ${events?.length || 0}`);
    
    // Group by Category
    const byCategory = {};
    events?.forEach(e => {
        byCategory[e.category] = (byCategory[e.category] || 0) + 1;
    });
    console.table(byCategory);

    // Group by Parsing Method (need to join validation or check staging)
    const { data: staged } = await supabase.from('raw_event_staging').select('parsing_method, status');
    const byMethod = {};
    staged?.forEach(s => {
        byMethod[s.parsing_method || 'unknown'] = (byMethod[s.parsing_method || 'unknown'] || 0) + 1;
    });
    console.log("Parsing Methods:");
    console.table(byMethod);
}

main();
